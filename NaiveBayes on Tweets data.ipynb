{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to implement a Naive Bayes classifier to predict whether a tweet was posted by a Republican or Democrat politician. The training data consist of about 13K tweets collected before the 2016 US presidential elections, There are about an equal number of Republican and Democrat tweets, and the tweets belong to three republican and three democrat twitter accounts. \n",
    "\n",
    "To represent each tweet, we will use a commonly used model in natural language processing called 'bag of words' model. A bag of words representation of a document (tweet here) consists of words and their frequencies in the document. The order of words is ignored.  \n",
    "\n",
    "There four main tasks.\n",
    "1. Tokenization: Parsing and converting the tweets to tokens. \n",
    "2. Feature matrix construction from the training data set\n",
    "3. Learning Naive Bayes parameters, priors and likelihoods, from the feature matrix.\n",
    "4. Using the learned NB model to predict the labels of the test data set (about 4K tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "This task consists of converting each tweet into a sequence of \"tokens\" that can be used as features. Tokens are essentially characters and character sequences obtained after using white space as a separator. A lot these are noise that we want to remove; some are words or other character sequences that are useful features. A python package called *NLTK* (natural language toolkit) contains several tokenizers, including one for tweets. We use that tokenizer; in addition we do the following:\n",
    "- remove stopwords. These are words that are frequently used in a language but do not carry any semantic information, e.g., the, an , a, this, is, was, etc.\n",
    "- make all tokens lower case (this is done by the tweet tokenizer)\n",
    "- removing twitter handles (again, done by the tweet tokenizer)\n",
    "- remove punctuations, http links\n",
    "\n",
    "Finally, we \"lemmatize\" the tokens. That means we convert different forms of a word to a common basic form, so that they can be recognized as the same work. E.g., vote, votes, voted would all be converted to vote; geese would be converted to goose,e tc. (There is a less sophisticated version of lemmatizer called a stemmer which just chops words to convert to the same base work; it doesn't work as well as a lemmatizer and we dont use it here.) There is a good description of the NLTK tokenizer [here](https://berkeley-stat159-f17.github.io/stat159-f17/lectures/11-strings/11-nltk..html).\n",
    "\n",
    "The output of this part is a cleaned up list of tokens for each tweet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\srira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\srira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "#\n",
    "# you may need to run the following\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set has two columns - screen_name and text (which is the raw tweet)\n",
    "\n",
    "## load tweets\n",
    "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)\n",
    "\n",
    "## screen_namee (accounts)\n",
    "#  democrat - hillary, time kaine, TheDemocrats\n",
    "# republicans - trunp, pence, GOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GOP', 'TheDemocrats', 'HillaryClinton', 'timkaine', 'mike_pence',\n",
       "       'realDonaldTrump'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['screen_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOP</td>\n",
       "      <td>RT @GOPconvention: #Oregon votes today. That m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheDemocrats</td>\n",
       "      <td>RT @DWStweets: The choice for 2016 is clear: W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>Trump's calling for trillion dollar tax cuts f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>.@TimKaine's guiding principle: the belief tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>timkaine</td>\n",
       "      <td>Glad the Senate could pass a #THUD / MilCon / ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      screen_name                                               text\n",
       "0             GOP  RT @GOPconvention: #Oregon votes today. That m...\n",
       "1    TheDemocrats  RT @DWStweets: The choice for 2016 is clear: W...\n",
       "2  HillaryClinton  Trump's calling for trillion dollar tax cuts f...\n",
       "3  HillaryClinton  .@TimKaine's guiding principle: the belief tha...\n",
       "4        timkaine  Glad the Senate could pass a #THUD / MilCon / ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13000</td>\n",
       "      <td>13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>12982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2217</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            screen_name                       text\n",
       "count             13000                      13000\n",
       "unique                6                      12982\n",
       "top     realDonaldTrump  MAKE AMERICA GREAT AGAIN!\n",
       "freq               2217                          4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13000</td>\n",
       "      <td>13000</td>\n",
       "      <td>13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>12982</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2217</td>\n",
       "      <td>4</td>\n",
       "      <td>6554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            screen_name                       text  label\n",
       "count             13000                      13000  13000\n",
       "unique                6                      12982      2\n",
       "top     realDonaldTrump  MAKE AMERICA GREAT AGAIN!  False\n",
       "freq               2217                          4   6554"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add labels\n",
    "# 1 for D's\n",
    "# 0 for R's\n",
    "tweets['label'] = tweets['screen_name'].str.contains('TheDemocrats|HillaryClinton|timkaine', regex=True)\n",
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has 13K tweets, and each of the two classes have about an equal number of tweets.\n",
    "\n",
    "Now we will define our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#\n",
    "#  Input : dataframe with a column names 'text' which contains raw tweets (one per row)\n",
    "#  Output: A list of lists of tokens corrsponding to the 'text' column\n",
    "#\n",
    "def tokenize_tweets2(tweets):\n",
    "    \"\"\"Given a df with tweets in 'text' col, this function return tokens as a list of lists\"\"\"\n",
    "\n",
    "    # apply tokenize to the 'text' coolumn in the tweets df\n",
    "    tweet_tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    tokens = tweets['text'].apply(tweet_tokenizer.tokenize)\n",
    "    \n",
    "    # filter\n",
    "    misc = ['rt', '’', '…', '—', 'u', '”', 'w', '“', '...', '️', 'http', 'https']\n",
    "    to_remove = nltk.corpus.stopwords.words('English') + list(string.punctuation) + misc\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = [[lemmatizer.lemmatize(token) for token in tw if token not in to_remove] for tw in tokens]      \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6446\n",
      "6554\n"
     ]
    }
   ],
   "source": [
    "all_tokens = tokenize_tweets2(tweets)\n",
    "tweets_dem=tweets[tweets['label']]\n",
    "tweets_rep=tweets[tweets['label']==0]\n",
    "token_dem=tokenize_tweets2(tweets_dem)\n",
    "token_rep=tokenize_tweets2(tweets_rep)\n",
    "print(len(token_dem))\n",
    "print(len(token_rep))\n",
    "#print(token_rep[:10])\n",
    "#print(token_rep[:10])\n",
    "#all_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's find the most common tokens, and we will use all tokens that at least occur 25 times as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hillary', 1159),\n",
       " ('trump', 1144),\n",
       " ('great', 749),\n",
       " ('clinton', 720),\n",
       " ('today', 709),\n",
       " ('make', 581),\n",
       " ('donald', 576),\n",
       " ('president', 564),\n",
       " ('day', 552),\n",
       " ('thank', 539),\n",
       " ('american', 512),\n",
       " ('new', 503),\n",
       " ('job', 503),\n",
       " ('u', 485),\n",
       " ('america', 480),\n",
       " ('people', 469),\n",
       " ('vote', 451),\n",
       " ('state', 442),\n",
       " ('get', 420),\n",
       " ('year', 415)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter([token for tokens in all_tokens for token in tokens])\n",
    "print(len(counts))\n",
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = [k for k in counts.keys() if counts.get(k) > 25]\n",
    "len(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_words are our features.\n",
    "Now let's construct a feature matrix from these top words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Martix Construction\n",
    "\n",
    "Compute feature matrix\n",
    "\n",
    "Now we will extract the features from the training data and construct a feature matrix. The bad news is this matrix can be very large. In our case it is about 13K X 1K, or about 13M x 4 bytes ~ 52M, which will easily fit in the RAM of your laptops, but the training set could have easily been 10x or 100x the current size, and the number of features 10x in which case you would be out of luck. The good news is this matrix is likely to be very sparse. In fact, each tweet is not likely to contain more than 10-20 tokens, so even if this matrix becomes very large, we would be okay if we use a sparse representation.\n",
    "\n",
    "In a sparse representation, only the non-zero entities and their indices are saved. Scipy provides [several formats](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html) for sparse matrices. \n",
    "\n",
    "To make it easier to estimate priors and likelihoods, we will construct two feature matrices - one for each for the two classes. For this, first we need to figure out how many data points are in each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = len(top_words)\n",
    "\n",
    "# set this to the correct values\n",
    "nTrainR = sum(tweets['label']==0)  # number of R (0) training points\n",
    "nTrainD =sum(tweets['label'])   # number of D (1) training points\n",
    "\n",
    "\n",
    "# create sparse feature matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "rfmat = csc_matrix((nTrainR, num_feat), dtype=int)\n",
    "dfmat = csc_matrix((nTrainD, num_feat), dtype=int)\n",
    "\n",
    "#\n",
    "# populate rfmat and dfmat with the counts of the features\n",
    "# Remember: all tokens are not features\n",
    "#\n",
    "# a function that might be useful is <list>.index() \n",
    "#\n",
    "row=[]\n",
    "cols=[]\n",
    "data=[]\n",
    "for index, item in enumerate(token_dem):\n",
    "    i = index\n",
    "    list_tmp = item\n",
    "    for index, item in enumerate(list_tmp):\n",
    "        if item in top_words:\n",
    "            row.append(i)\n",
    "            cols.append(top_words.index(item))\n",
    "            data.append(1)\n",
    "            \n",
    "dfmat = csc_matrix((data, (row, cols)), shape=(nTrainD, num_feat))\n",
    "row1=[]\n",
    "cols1=[]\n",
    "data1=[]\n",
    "for index, item in enumerate(token_rep):\n",
    "    i = index\n",
    "    list_tmp = item\n",
    "    for index, item in enumerate(list_tmp):\n",
    "        if item in top_words:\n",
    "            row1.append(i)\n",
    "            cols1.append(top_words.index(item))\n",
    "            data1.append(1)\n",
    "            \n",
    "rfmat = csc_matrix((data1, (row1, cols1)), shape=(nTrainR, num_feat))\n",
    "#rfmat\n",
    "#print(dfmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Naive Bayes Model Parameters\n",
    "\n",
    "compute log priors\n",
    "\n",
    "compute log likelihoods using Laplace smoothing\n",
    "\n",
    "Now we can compute the model parameters, this is, the likelihoods and priors for the two classes. As we discussed in class, since the probabilities can be very small numbers, we will compute log likelihoods and log priors. Aslo use Laplace (aka add one) smoothing.\n",
    "\n",
    "To sum a matrix column, you can use something like dfmat[:,i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior for democratic: -0.7014895740682907\n",
      "prior for republican: -0.6848738071849139\n",
      "likelihood for democratic: [-4.879083404688273, -4.70970883426038, -6.862604824489369, -5.200898483509512, -6.8149767755001145, -6.054388314144637, -7.049816366577516, -5.116591377049518, -6.515733880647257, -5.112345086168067, -6.605884977641555, -6.386522149167251, -5.894045664069457, -7.243972381018473, -5.417121591979148, -7.531654453470254, -8.091270241405677, -5.8487890724813365, -6.8385072729103085, -6.912615245064031, -7.742963547137461, -5.434513334691017, -6.417293807834005, -7.485134437835361, -6.938590731467292, -7.685805133297513, -4.74723127358347, -6.550825200458528, -6.327681649144318, -7.357301066325476]\n",
      "likelihood for republican: [-5.4962423605355735, -4.65913449105412, -7.59522849828838, -4.716621581971801, -7.320791652586619, -5.9445476273202305, -7.04315991598834, -5.9857905858542795, -6.235602384250651, -6.291172235405461, -6.95614853899871, -6.350012735428395, -5.727483122082403, -10.53966747745482, -7.706454133398604, -7.831617276352611, -7.706454133398604, -6.145218322782381, -7.649295719558656, -6.984319415965406, -7.59522849828838, -5.282172105427039, -7.243830611450491, -7.1723716474683465, -7.320791652586619, -8.237082384460775, -5.046606034114272, -7.59522849828838, -6.755477843536559, -7.831617276352611]\n"
     ]
    }
   ],
   "source": [
    "# compute log priors\n",
    "\n",
    "import math\n",
    "log_p_rep=math.log(nTrainR/len(tweets))\n",
    "log_p_dem=math.log(nTrainD/len(tweets))\n",
    "p_likel_dem=[]\n",
    "p_likel_rep=[]\n",
    "\n",
    "# compute log likelihoods\n",
    "\n",
    "for i in range(0,927):\n",
    "    p_likel_dem.append(math.log((dfmat[:,i].sum()+1)/(dfmat.sum()+2)))\n",
    "    p_likel_rep.append(math.log((rfmat[:,i].sum()+1)/(rfmat.sum()+2)))\n",
    "\n",
    "print(\"prior for democratic:\",log_p_dem)\n",
    "print(\"prior for republican:\",log_p_rep)\n",
    "print(\"likelihood for democratic:\",p_likel_dem[:30])\n",
    "print(\"likelihood for republican:\",p_likel_rep[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test Set\n",
    "\n",
    "Now we have a trained Naive Bayes classifier. We will load the test data set and make the predictions. Note: If a token is not a feature, ignore it. \n",
    "\n",
    "Load test data and tokenize\n",
    "\n",
    "Using the trained NB classifier predict the labels\n",
    "\n",
    "Calculate accuracy, recall, and precision of your predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test labels: [True, False, False, True, True, False, False, True, False, True, False, True, False, True, True, False, True, True, False, True, True, False, True, True, False, True, True, False, True, False]\n",
      "Accuracy: 0.8120055839925546\n",
      "Recall: 0.8215271389144434\n",
      "Precision: 0.8096101541251133\n"
     ]
    }
   ],
   "source": [
    "#Load test data and tokenize\n",
    "\n",
    "tweets_test = pd.read_csv(\"tweets_test.csv\", na_filter=False)\n",
    "test_res = tweets_test['screen_name'].str.contains('TheDemocrats|HillaryClinton|timkaine', regex=True)\n",
    "test_tokens = tokenize_tweets2(tweets_test)\n",
    "#tweets_test.describe()\n",
    "\n",
    "#NB Classifier to predict labels\n",
    "\n",
    "list_tmp1=[]\n",
    "poster_res=[]\n",
    "for index, item in enumerate(test_tokens):\n",
    "    i = index\n",
    "    list_tmp1 = item\n",
    "    dem_prob=0\n",
    "    rep_prob=0\n",
    "    for index, item in enumerate(list_tmp1):\n",
    "        if item in top_words:\n",
    "            col2=top_words.index(item)\n",
    "            dem_prob=dem_prob+p_likel_dem[col2]\n",
    "            rep_prob=rep_prob+p_likel_rep[col2]\n",
    "        else:\n",
    "            continue\n",
    "    dem_prob=dem_prob+log_p_dem\n",
    "    rep_prob=rep_prob+log_p_rep\n",
    "    if dem_prob>rep_prob:\n",
    "        poster_res.append(True)\n",
    "    else:\n",
    "        poster_res.append(False)\n",
    "\n",
    "print(\"Test labels:\",poster_res[:30])\n",
    "\n",
    "#Accuracy,recall and precision\n",
    "\n",
    "i,TP,TN,FP,FN=0,0,0,0,0\n",
    "for res in poster_res:\n",
    "    if (test_res[i]==True and res==True):\n",
    "        TP+=1\n",
    "    elif (test_res[i]==True and res==False):\n",
    "        FN+=1\n",
    "        \n",
    "    elif (test_res[i]==False and res==True):\n",
    "        FP+=1\n",
    "        \n",
    "    elif (test_res[i]==False and res==False):\n",
    "        TN+=1\n",
    "    i=i+1\n",
    "    acc=(TP+TN)/(TP+TN+FP+FN)\n",
    "    rec=TP/(TP+FP)\n",
    "    prec=TP/(TP+FN)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"Precision:\", prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of features with top ten likelihoods for each of the two classes. Look into things like:\n",
    "What is the likelihood for 'hillary', that is, P(hillary|class)? \n",
    "Is it in the top ten? \n",
    "How important is it in this classification problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likelihoods for republicans: ['state', 'job', 'indiana', 'day', 'new', 'today', 'thank', 'great', 'hillary', 'clinton']\n",
      "Top likelihoods for democrats: ['one', 'vote', 'u', 'make', 'american', 'today', 'president', 'donald', 'hillary', 'trump']\n",
      "P(Hillary|democrat) , P(Hillary|republic)  -4.225291174478937 -4.162940529556193\n"
     ]
    }
   ],
   "source": [
    "#Democrats\n",
    "\n",
    "top_likel_dem=np.argsort(p_likel_dem)[-10:]\n",
    "top_likel_demtokens=[]\n",
    "for index,item in enumerate(top_likel_dem):\n",
    "    top_likel_demtokens.append(top_words[item])\n",
    "    \n",
    "#Republicans\n",
    "\n",
    "top_likel_rep=np.argsort(p_likel_rep)[-10:]\n",
    "top_likel_reptokens=[]\n",
    "for index,item in enumerate(top_likel_rep):\n",
    "    top_likel_reptokens.append(top_words[item])\n",
    "    \n",
    "print(\"Top likelihoods for republicans:\", top_likel_reptokens)\n",
    "print(\"Top likelihoods for democrats:\", top_likel_demtokens)\n",
    "\n",
    "#Importance of Hillary\n",
    "\n",
    "hillary=top_words.index(\"hillary\")\n",
    "rep_hillary=p_likel_dem[hillary]\n",
    "dem_hillary=p_likel_rep[hillary]\n",
    "print(\"P(Hillary|democrat) , P(Hillary|republic) \",rep_hillary,dem_hillary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How important are the priors in this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When likelihoods are negligible, closer to zero or zero, they determine the posterior\n",
    "2. When tweets do not contain any of the top_words i.e features, priors determine the probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of the test set without Laplace smoothing and compare with the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test labels: [True, False, False, True, True, True, False, True, False, True, False, True, False, True, True, False, True, True, False, True, True, False, True, True, False, True, True, False, True, False]\n",
      "Accuracy: 0.6644951140065146\n",
      "Recall: 0.6552845528455284\n",
      "Precision: 0.7307343608340888\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "p_likel_dem_wolaplace=[]\n",
    "p_likel_rep_wolaplace=[]\n",
    "\n",
    "# compute log likelihoods\n",
    "\n",
    "for index,item in enumerate(top_words):\n",
    "    if dfmat[:,index].sum() == 0:\n",
    "        p_likel_dem_wolaplace.append(0)\n",
    "    else:\n",
    "        p_likel_dem_wolaplace.append(math.log((dfmat[:,index].sum())/(dfmat.sum())))\n",
    "        \n",
    "for index,item in enumerate(top_words):\n",
    "    if rfmat[:,index].sum()==0:\n",
    "        p_likel_rep_wolaplace.append(0)\n",
    "    else:\n",
    "        p_likel_rep_wolaplace.append(math.log((rfmat[:,index].sum())/(rfmat.sum())))\n",
    "        \n",
    "#print(\"likelihood for democratic without laplace:\",p_likel_dem_wolaplace[:30])\n",
    "#print(\"likelihood for republican:\",p_likel_rep_wolaplace[:30])\n",
    "\n",
    "poster_res=[]\n",
    "for index, item in enumerate(test_tokens):\n",
    "    i = index\n",
    "    list_tmp1 = item\n",
    "    dem_prob=0\n",
    "    rep_prob=0\n",
    "    for index, item in enumerate(list_tmp1):\n",
    "        if item in top_words:\n",
    "            col2=top_words.index(item)\n",
    "            dem_prob=dem_prob+p_likel_dem_wolaplace[col2]\n",
    "            rep_prob=rep_prob+p_likel_rep_wolaplace[col2]\n",
    "        else:\n",
    "            continue\n",
    "    dem_prob=dem_prob+log_p_dem\n",
    "    rep_prob=rep_prob+log_p_rep\n",
    "    if dem_prob>rep_prob:\n",
    "        poster_res.append(True)\n",
    "    else:\n",
    "        poster_res.append(False)\n",
    "print(\"Test labels:\",poster_res[:30])\n",
    "\n",
    "#Accuracy,recall and precision\n",
    "i,TP,TN,FP,FN=0,0,0,0,0\n",
    "for res in poster_res:\n",
    "    if (test_res[i]==True and res==True):\n",
    "        TP+=1\n",
    "    elif (test_res[i]==True and res==False):\n",
    "        FN+=1\n",
    "        \n",
    "    elif (test_res[i]==False and res==True):\n",
    "        FP+=1\n",
    "        \n",
    "    elif (test_res[i]==False and res==False):\n",
    "        TN+=1\n",
    "    i=i+1\n",
    "    acc=(TP+TN)/(TP+TN+FP+FN)\n",
    "    rec=TP/(TP+FP)\n",
    "    prec=TP/(TP+FN)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"Precision:\", prec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
